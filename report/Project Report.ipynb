{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Satellite Image Classification using ConvNet, Keras and TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Every minute, the world loses an area of forest the size of 48 football fields. And deforestation in the Amazon Basin accounts for the largest share, contributing to reduced biodiversity, habitat loss, climate change, and other devastating effects. But better data about the location of deforestation and human encroachment on forests can help governments and local stakeholders respond more quickly and effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Labels\n",
    "Datasets comprise of 17 labels.<br>\n",
    "haze, primary, agriculture, clear, water, habitation, road, cultivation, slash_burn, cloudy, partly_cloudy, conventional_mine, bare_ground, artisinal_mine, blooming, selective_logging, blow_down\n",
    "\n",
    "![](img\\chips.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Networks (ConvNet/CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enviornment Setup\n",
    "(Verify these, probably will need to add remove some packages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install\n",
    "conda create -n keras python=3.5 jupyter<br>\n",
    "activate keras<br>\n",
    "conda install theano<br>\n",
    "conda install mingw libpython<br>\n",
    "pip install tensorflow<br>\n",
    "pip install keras<br>\n",
    "pip install scikit-learn<br>\n",
    "pip install pillow<br>\n",
    "pip install h5py<br>\n",
    "pip install tensorflow-gpu<br>\n",
    "pip install imagenet_utils<br>\n",
    "activate keras\n",
    "### Open CV\n",
    "conda install -c menpo opencv3\n",
    "\n",
    "### Verify\n",
    "python -c \"from keras import backend; print(backend._BACKEND)\"\n",
    "\n",
    "### Config\n",
    "python -c \"import os; print(os.path.expanduser('~') + '\\.keras\\\\keras.json')\"\n",
    "\n",
    "### Verify GPU\n",
    "python -c \"import tensorflow as tf; sess = tf.Session(); hello = tf.constant('Hello, TensorFlow!'); print(sess.run('hello'))\"\n",
    "\n",
    "### Run\n",
    "activate keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "Download and unzip following datasets from Kaggle. Note for this project we'll be using jpg instead of tif. As processing high resolution tif is computationally expensive. With jpg datasets we can achieve satisfactory results, 96% accuracy and top 15% on Kaggle Leader board.<br>\n",
    "\n",
    "test-jpg.tar.7z<br>\n",
    "train-jpg.tar.7z\n",
    "\n",
    "URL: https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import cv2\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import time # Timing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Serialize train and test datasets in hdf5.\n",
    "Create onehot encoding for train labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading image: c:/data/amazon/test-jpg/test_0.jpg\n",
      "reading image: c:/data/amazon/test-jpg/test_5000.jpg\n",
      "reading image: c:/data/amazon/test-jpg/test_10000.jpg\n",
      "reading image: c:/data/amazon/test-jpg/test_15000.jpg\n",
      "reading image: c:/data/amazon/test-jpg/test_20000.jpg\n",
      "reading image: c:/data/amazon/test-jpg/test_25000.jpg\n",
      "reading image: c:/data/amazon/test-jpg/test_30000.jpg\n",
      "reading image: c:/data/amazon/test-jpg/test_35000.jpg\n",
      "reading image: c:/data/amazon/test-jpg/test_40000.jpg\n",
      "Saving file: test-dataset-128.h5\n",
      "Time elapsed: 85.76890587806702 seconds\n"
     ]
    }
   ],
   "source": [
    "test_output_file = 'test-dataset-128.h5'\n",
    "train_output_file = \"train-dataset-128.h5\"\n",
    "\n",
    "test_image_path = 'c:/data/amazon/test-jpg'\n",
    "train_image_path = 'c:/data/amazon/train-jpg'\n",
    "\n",
    "train_max_image_idx = 40478\n",
    "test_max_image_idx = 40668\n",
    "\n",
    "train_csv = 'c:/data/amazon/train.csv'\n",
    "\n",
    "image_resize = (128,128) # Resize images\n",
    "\n",
    "x = []\n",
    "\n",
    "start_time_data_prep = time.time()\n",
    "\n",
    "for i in range(0, test_max_image_idx + 1):\n",
    "    img = test_image_path + \"/test_\" + str(i) + \".jpg\"\n",
    "    if i % 5000 == 0:\n",
    "        print(\"reading image: {}\".format(img))\n",
    "    img = cv2.imread(img)\n",
    "    img = cv2.resize(img,image_resize)\n",
    "    # img = img.transpose((2,0,1))\n",
    "    x.append(img)\n",
    "\n",
    "print('Saving file: {}'.format(test_output_file))\n",
    "x = np.array(x)\n",
    "f = h5py.File(test_output_file)\n",
    "f['x'] = x\n",
    "f.close()\n",
    "\n",
    "print('Time elapsed: {} seconds'.format(time.time()-start_time_data_prep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (40479, 2)\n",
      "Labels: ['haze', 'primary', 'agriculture', 'clear', 'water', 'habitation', 'road', 'cultivation', 'slash_burn', 'cloudy', 'partly_cloudy', 'conventional_mine', 'bare_ground', 'artisinal_mine', 'blooming', 'selective_logging', 'blow_down']\n",
      "reading image: c:/data/amazon/train-jpg/train_0.jpg\n",
      "reading image: c:/data/amazon/train-jpg/train_5000.jpg\n",
      "reading image: c:/data/amazon/train-jpg/train_10000.jpg\n",
      "reading image: c:/data/amazon/train-jpg/train_15000.jpg\n",
      "reading image: c:/data/amazon/train-jpg/train_20000.jpg\n",
      "reading image: c:/data/amazon/train-jpg/train_25000.jpg\n",
      "reading image: c:/data/amazon/train-jpg/train_30000.jpg\n",
      "reading image: c:/data/amazon/train-jpg/train_35000.jpg\n",
      "reading image: c:/data/amazon/train-jpg/train_40000.jpg\n",
      "Saving file: train-dataset-128.h5\n",
      "Time elapsed: 102.22684693336487 seconds\n"
     ]
    }
   ],
   "source": [
    "## Train dataset\n",
    "train_output_file = \"train-dataset-128.h5\"\n",
    "train_image_path = 'c:/data/amazon/train-jpg'\n",
    "train_max_image_idx = 40478\n",
    "train_csv = 'c:/data/amazon/train.csv'\n",
    "\n",
    "image_resize = (128,128) # Resize images\n",
    "\n",
    "start_time_data_prep = time.time()\n",
    "\n",
    "df = pd.read_csv(train_csv)\n",
    "print('Training dataset shape: {}'.format(df.shape))\n",
    "df.head()\n",
    "\n",
    "# Build list with unique labels\n",
    "label_list = []\n",
    "for tag_str in df.tags.values:\n",
    "    labels = tag_str.split(' ')\n",
    "    for label in labels:\n",
    "        if label not in label_list:\n",
    "            label_list.append(label)\n",
    "\n",
    "print('Labels: {}'.format(label_list))\n",
    "\n",
    "# Add onehot features for every label\n",
    "for label in label_list:\n",
    "    df[label] = df['tags'].apply(lambda x: 1 if label in x.split(' ') else 0)\n",
    "\n",
    "# Display head\n",
    "df.head()\n",
    "\n",
    "y = np.array(df.ix[:,2:])\n",
    "#print(y.shape)\n",
    "\n",
    "x = []\n",
    "\n",
    "for i in range(0, train_max_image_idx + 1):\n",
    "    img = train_image_path + \"/train_\" + str(i) + \".jpg\"\n",
    "    if i % 5000 == 0:\n",
    "        print(\"reading image: {}\".format(img))\n",
    "    img = cv2.imread(img)\n",
    "    img = cv2.resize(img,image_resize)\n",
    "    # img = img.transpose((2,0,1))\n",
    "    x.append(img)\n",
    "\n",
    "print('Saving file: {}'.format(train_output_file))\n",
    "x = np.array(x)\n",
    "f = h5py.File(train_output_file)\n",
    "f['x'] = x\n",
    "f['y'] = y\n",
    "f.close()\n",
    "\n",
    "print('Time elapsed: {} seconds'.format(time.time()-start_time_data_prep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One hot encoding for the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>tags</th>\n",
       "      <th>haze</th>\n",
       "      <th>primary</th>\n",
       "      <th>agriculture</th>\n",
       "      <th>clear</th>\n",
       "      <th>water</th>\n",
       "      <th>habitation</th>\n",
       "      <th>road</th>\n",
       "      <th>cultivation</th>\n",
       "      <th>slash_burn</th>\n",
       "      <th>cloudy</th>\n",
       "      <th>partly_cloudy</th>\n",
       "      <th>conventional_mine</th>\n",
       "      <th>bare_ground</th>\n",
       "      <th>artisinal_mine</th>\n",
       "      <th>blooming</th>\n",
       "      <th>selective_logging</th>\n",
       "      <th>blow_down</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_0</td>\n",
       "      <td>haze primary</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_1</td>\n",
       "      <td>agriculture clear primary water</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2</td>\n",
       "      <td>clear primary</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_3</td>\n",
       "      <td>clear primary</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_4</td>\n",
       "      <td>agriculture clear habitation primary road</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_name                                       tags  haze  primary  \\\n",
       "0    train_0                               haze primary     1        1   \n",
       "1    train_1            agriculture clear primary water     0        1   \n",
       "2    train_2                              clear primary     0        1   \n",
       "3    train_3                              clear primary     0        1   \n",
       "4    train_4  agriculture clear habitation primary road     0        1   \n",
       "\n",
       "   agriculture  clear  water  habitation  road  cultivation  slash_burn  \\\n",
       "0            0      0      0           0     0            0           0   \n",
       "1            1      1      1           0     0            0           0   \n",
       "2            0      1      0           0     0            0           0   \n",
       "3            0      1      0           0     0            0           0   \n",
       "4            1      1      0           1     1            0           0   \n",
       "\n",
       "   cloudy  partly_cloudy  conventional_mine  bare_ground  artisinal_mine  \\\n",
       "0       0              0                  0            0               0   \n",
       "1       0              0                  0            0               0   \n",
       "2       0              0                  0            0               0   \n",
       "3       0              0                  0            0               0   \n",
       "4       0              0                  0            0               0   \n",
       "\n",
       "   blooming  selective_logging  blow_down  \n",
       "0         0                  0          0  \n",
       "1         0                  0          0  \n",
       "2         0                  0          0  \n",
       "3         0                  0          0  \n",
       "4         0                  0          0  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorBoard Graph\n",
    "VGG16 pre-trained model frozen upto conv4. Set the first 15 layers (up to the conv4) to non-trainable (weights will not be updated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/tb-graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Reshape\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import applications\n",
    "from keras.optimizers import adam\n",
    "import h5py\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to load traing dataset for a given batch size and train/validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_train_dataset(dataset, random_state, batch_size=-1, test_size=0.2):\n",
    "    '''\n",
    "    batch_size=-1 returns all\n",
    "    '''\n",
    "    f = h5py.File(dataset)\n",
    "    if batch_size == -1:\n",
    "        x = f['x'].value\n",
    "        y = f['y'].value\n",
    "    else:\n",
    "        x = f['x'][:batch_size,]\n",
    "        y = f['y'][:batch_size,]\n",
    "\n",
    "    f.close()\n",
    "    x_train , x_test, y_train, y_test = train_test_split(x,y,test_size=test_size, random_state=random_state)\n",
    "        \n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to load test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_test_dataset(dataset):\n",
    "    f = h5py.File(dataset)\n",
    "    x = f['x'].value   #[0:100,]\n",
    "    f.close()\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Save Bottleneck Features from VGG16 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "batch_size = 8\n",
    "bottleneck_features_train_path = 'bottleneck_features_train_128_vgg.npy'\n",
    "bottleneck_features_validation_path = 'bottleneck_features_validation_128_vgg.npy'\n",
    "# path to the model weights files.\n",
    "top_model_weights_path = 'bottleneck_fc_model_128_vgg.h5'\n",
    "# dimensions of our images.\n",
    "input_shape = (128,128,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Bottleneck Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_bottleneck_features():\n",
    "    # build the VGG16 network\n",
    "    # First time it will take longer, as it downloads the weights.\n",
    "    model = applications.VGG16(include_top=False, weights='imagenet')\n",
    "    model.summary()\n",
    "    start_time = time.time()\n",
    "\n",
    "    bottleneck_features_train = model.predict(\n",
    "        x_train, batch_size = batch_size, verbose=1)\n",
    "\n",
    "    np.save(open(bottleneck_features_train_path, 'wb'),\n",
    "            bottleneck_features_train)\n",
    "\n",
    "    bottleneck_features_validation = model.predict(\n",
    "        x_test, batch_size = batch_size, verbose=1)\n",
    "    np.save(open(bottleneck_features_validation_path, 'wb'),\n",
    "            bottleneck_features_validation)\n",
    "    print('save_bottleneck_features(): Time elapsed: {} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Top Model\n",
    "Train fully conected model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_top_model():\n",
    "    start_time = time.time()\n",
    "    train_data = np.load(open(bottleneck_features_train_path, 'rb'))\n",
    "    train_labels = y_train\n",
    "    validation_data = np.load(open(bottleneck_features_validation_path, 'rb'))\n",
    "    validation_labels = y_test\n",
    "\n",
    "    print('train_data.shape[1:]: {}'.format(train_data.shape[1:]))\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(17, activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    #### Update this if using weights from previous run ####\n",
    "    # model.load_weights(top_model_weights_path)\n",
    "    model.fit(train_data, train_labels,\n",
    "              epochs=epochs,\n",
    "              batch_size=batch_size,\n",
    "              validation_data=(validation_data, validation_labels))\n",
    "    model.save_weights(top_model_weights_path)\n",
    "    print('train_top_model(): Time elapsed: {} seconds'.format(time.time()-start_time_data_prep))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Putting them together and train full model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_full_model():\n",
    "    start_time = time.time()\n",
    "    # build the VGG16 network\n",
    "    base_model = applications.VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    print('Model loaded.')\n",
    "    # base_model.summary()\n",
    "    # build a classifier model to put on top of the convolutional model\n",
    "    top_model = Sequential()\n",
    "    top_model.add(Flatten(input_shape=(4,4,512)))\n",
    "    top_model.add(Dense(256, activation='relu'))\n",
    "    top_model.add(Dropout(0.5))\n",
    "    top_model.add(Dense(17, activation='sigmoid'))\n",
    "\n",
    "    # note that it is necessary to start with a fully-trained\n",
    "    # classifier, including the top classifier,\n",
    "    # in order to successfully do fine-tuning\n",
    "    top_model.load_weights(top_model_weights_path)\n",
    "\n",
    "    # top_model.summary()\n",
    "\n",
    "    # add the model on top of the convolutional base\n",
    "    model = Model(inputs= base_model.input, outputs= top_model(base_model.output))\n",
    "\n",
    "    # set the first 15 layers (up to the last conv block)\n",
    "    # to non-trainable (weights will not be updated)\n",
    "    for layer in model.layers[:15]:\n",
    "        # print(layer.name)\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    #### Update following if using weights from previous run ####\n",
    "    #model.load_weights('weights-model-07.01-0.95972.hdf5')\n",
    "\n",
    "    # compile the model with adam optimizer\n",
    "    # and a very slow learning rate.\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                optimizer=adam(lr=1e-4),\n",
    "                metrics=['accuracy'])\n",
    "                \n",
    "    x_train, x_test, y_train, y_test = load_train_dataset(random_state=random_state, dataset=train_dataset, test_size=test_size)\n",
    "\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test  = x_test.astype('float32')\n",
    "\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "\n",
    "    tbCallBack = TensorBoard(log_dir='graph', histogram_freq=0, write_graph=True, write_images=False, embeddings_freq=0)\n",
    "    check = ModelCheckpoint(\"weights-model-07.{epoch:02d}-{val_acc:.5f}.hdf5\", monitor='val_acc', verbose=1, \n",
    "                        save_best_only=True, save_weights_only=True, mode='auto')\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,callbacks=[check],validation_data=(x_test,y_test))\n",
    "\n",
    "\n",
    "    ## Predict\n",
    "    print('Generate Predictions...')\n",
    "    x_test =  load_test_dataset(test_dataset)\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_test /= 255.\n",
    "\n",
    "    best_threshold = [0.2] * 17\n",
    "\n",
    "    # print(\"best_threshold: {}\".format(best_threshold))\n",
    "\n",
    "    pred = model.predict(x_test, verbose=1, batch_size=8)\n",
    "    # print(pred)\n",
    "    print(pred.shape)\n",
    "\n",
    "    classes = ['haze',\n",
    "            'primary',\n",
    "            'agriculture',\n",
    "            'clear',\n",
    "            'water',\n",
    "            'habitation',\n",
    "            'road',\n",
    "            'cultivation',\n",
    "            'slash_burn',\n",
    "            'cloudy',\n",
    "            'partly_cloudy',\n",
    "            'conventional_mine',\n",
    "            'bare_ground',\n",
    "            'artisinal_mine',\n",
    "            'blooming',\n",
    "            'selective_logging',\n",
    "            'blow_down']\n",
    "\n",
    "    y_pred = []\n",
    "\n",
    "    ##text=List of strings to be written to file\n",
    "    with open(submission_file,'w') as file:\n",
    "        file.write(\"image_name,tags\")\n",
    "        file.write('\\n')\n",
    "\n",
    "        for i in range(pred.shape[0]):\n",
    "            y_pred = np.array([1 if pred[i, j] >= best_threshold[j] else 0 for j in range(pred.shape[1])])\n",
    "            # print(y_pred)\n",
    "\n",
    "            # extracting actual class name\n",
    "            y_pred = [classes[i] for i in range(17) if y_pred[i] == 1]\n",
    "            y_pred = \" \".join([str(item) for item in y_pred])\n",
    "            # print(y_pred)\n",
    "            line = \"test_{},{}\".format(i, y_pred)\n",
    "            file.write(line)\n",
    "            file.write('\\n')\n",
    "    \n",
    "    print('train_full_model(): Time elapsed: {} seconds'.format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random_state = 101\n",
    "train_dataset = 'train-dataset-128.h5'\n",
    "test_size = 0.3 # for the train/test split\n",
    "submission_file = 'model-07-submission.csv'\n",
    "\n",
    "x_train, x_test, y_train, y_test = load_train_dataset(dataset=train_dataset, \n",
    "                                                      random_state=random_state, \n",
    "                                                      test_size=test_size)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test  = x_test.astype('float32')\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Save bottleneck features to npy arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "28335/28335 [==============================] - 1269s  \n",
      "12144/12144 [==============================] - 540s   \n",
      "save_bottleneck_features(): Time elapsed: 1816.2711477279663 seconds\n"
     ]
    }
   ],
   "source": [
    "save_bottleneck_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Top Model with the Bottleneck Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data.shape[1:]: (4, 4, 512)\n",
      "Train on 28335 samples, validate on 12144 samples\n",
      "Epoch 1/1\n",
      "28335/28335 [==============================] - 65s - loss: 0.2107 - acc: 0.9212 - val_loss: 0.1816 - val_acc: 0.9319\n",
      "train_top_model(): Time elapsed: 77.15817737579346 seconds\n"
     ]
    }
   ],
   "source": [
    "# Ideally you should run 10 epochs\n",
    "# If you are re-running, make sure to load the \n",
    "# weights from last run\n",
    "epochs = 1\n",
    "# Adjust this per your HW (from 8-32)\n",
    "batch_size = 8\n",
    "\n",
    "train_top_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Full Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 128, 128, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 128, 128, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 128, 128, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 64, 64, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 32, 32, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 16, 16, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 16, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 8, 8, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 8, 8, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         \n",
      "_________________________________________________________________\n",
      "sequential_5 (Sequential)    (None, 17)                2101777   \n",
      "=================================================================\n",
      "Total params: 16,816,465\n",
      "Trainable params: 9,181,201\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n",
      "Train on 28335 samples, validate on 12144 samples\n",
      "Epoch 1/1\n",
      "28328/28335 [============================>.] - ETA: 0s - loss: 0.2339 - acc: 0.9060Epoch 00000: val_acc improved from -inf to 0.90660, saving model to weights-model-07.00-0.90660.hdf5\n",
      "28335/28335 [==============================] - 2296s - loss: 0.2339 - acc: 0.9060 - val_loss: 0.2071 - val_acc: 0.9066\n",
      "Predictions...\n",
      "40669/40669 [==============================] - 1836s  \n",
      "(40669, 17)\n",
      "train_full_model(): Time elapsed: 4221.1407651901245 seconds\n"
     ]
    }
   ],
   "source": [
    "# Ideally you should run 10 epochs\n",
    "# If you are re-running, make sure to load the \n",
    "# weights from last run\n",
    "epochs = 1\n",
    "# Adjust this per your HW (from 8-32)\n",
    "batch_size = 8\n",
    "\n",
    "random_state = 55\n",
    "train_dataset = 'train-dataset-128.h5'\n",
    "test_dataset = 'test-dataset-128.h5'\n",
    "test_size = 0.3 # for the train/test split\n",
    "submission_file = 'model-07-submission.csv'\n",
    "\n",
    "train_full_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Above, only demonstarted 1 epoch. You should run 5-10 epochs to achieve accuracy of 0.96 and above. Changing the random_state, test_size should achieve higher accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data was provided in 2 formats. <br>\n",
    "High Resolution: TIF in 64 bit resolution 256 x 256 x 4 (Red, Green, Blue, near infrared channels)<br>\n",
    "Low Resolution: JPG in 32 bit resolution 256 x 256 x 3 (Red, Green, Blue channels)<br>\n",
    "\n",
    "We selected the JPG dataset and downsized the images to 128 x 128 x 3, for faster modeling as processing high resolution is computationaly expensive. <br>\n",
    "With the downsized images (1/10 of the data provided) we were able to secure top 14% on Kaggle Leadership board competing 318 teams. We are confident the model will achieve higher accuracies with high resolution images. We defer that for future improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/kaggle-board.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. [Planet: Understanding the Amazon from Space](https://www.kaggle.com/c/planet-understanding-the-amazon-from-space)\n",
    "2. [Transferring Pre-Trained Deep CNNs for Remote Scene Classification with General Features Learned from Linear PCA Network](http://www.mdpi.com:8080/2072-4292/9/3/225)\n",
    "2. [Building powerful image classification models using very little data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)\n",
    "3. [Deep Gold: Using Convolution Networks to Find Minerals](https://hackernoon.com/deep-gold-using-convolution-networks-to-find-minerals-aafdb37355df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
